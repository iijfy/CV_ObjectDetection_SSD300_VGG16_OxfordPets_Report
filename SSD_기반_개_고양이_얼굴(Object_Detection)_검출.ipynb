{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¶ğŸ± SSD ê¸°ë°˜ ê°œ/ê³ ì–‘ì´ ì–¼êµ´(Object Detection) ê²€ì¶œ\n",
        "\n",
        "## ğŸ“Œ 1. í”„ë¡œì íŠ¸ ëª©ì \n",
        "Oxford-IIIT Pet ë°ì´í„°ì…‹ì˜ ë°”ìš´ë”©ë°•ìŠ¤(XML) ì–´ë…¸í…Œì´ì…˜ì„ ì´ìš©í•´,\n",
        "ì´ë¯¸ì§€ì—ì„œ ê°œ/ê³ ì–‘ì´ì˜ ì–¼êµ´(Face) ì˜ì—­ì„ ê²€ì¶œí•˜ëŠ” Object Detection ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì´ë‹¤.\n",
        "\n",
        "ë¶„ë¥˜(Classification)ê°€ \"ì´ë¯¸ì§€ 1ì¥ â†’ ë¼ë²¨ 1ê°œ\"ë¼ë©´,\n",
        "íƒì§€(Object Detection)ëŠ” \"ì´ë¯¸ì§€ 1ì¥ â†’ (ë°”ìš´ë”©ë°•ìŠ¤ ì—¬ëŸ¬ ê°œ + í´ë˜ìŠ¤)\"ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•œë‹¤.\n",
        "ë”°ë¼ì„œ ëª¨ë¸ì€ ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ë™ì‹œì— ì˜í•´ì•¼ í•œë‹¤.\n",
        "\n",
        "- ì–´ë””ì— ìˆëŠ”ê°€(ìœ„ì¹˜): ë°•ìŠ¤ë¥¼ ì •í™•íˆ ë§ì¶”ê¸°\n",
        "- ë¬´ì—‡ì¸ê°€(ë¶„ë¥˜): ë°•ìŠ¤ê°€ catì¸ì§€ dogì¸ì§€ ë§ì¶”ê¸°\n",
        "\n",
        "\n",
        "## 2. ë°ì´í„°\n",
        "- ë°ì´í„°ì…‹: The Oxford-IIIT Pet Dataset (Kaggle mirror ì‚¬ìš©)\n",
        "- êµ¬ì„±\n",
        "  - images: ì›ë³¸ ì´ë¯¸ì§€(.jpg)\n",
        "  - annotations/xmls: ë°”ìš´ë”©ë°•ìŠ¤ ì–´ë…¸í…Œì´ì…˜(.xml)\n",
        "  - trainval.txt / test.txt: í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í•  ì •ë³´\n",
        "- ì‹¤ì œ í™•ì¸ ê²°ê³¼\n",
        "  - Train/Validation ë°ì´í„° ìˆ˜: 3680\n",
        "  - Test ë°ì´í„° ìˆ˜: 3669\n",
        "  - XML íŒŒì¼ ê°œìˆ˜: 3686\n",
        "\n",
        "\n",
        "## 3. ì „ì²´ ë¡œë“œë§µ\n",
        "- ëª¨ë¸: SSD300(VGG16 backbone)\n",
        "  - SSDëŠ” â€œì‹¤ì‹œê°„(ë¹ ë¥¸)â€ íƒì§€ ëª¨ë¸ ê³„ì—´ë¡œ êµ¬í˜„ì´ í‘œì¤€í™”ë˜ì–´ ìˆê³ , torchvisionì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì œê³µëœë‹¤.\n",
        "\n",
        "- Full Fine-tuning\n",
        "  - ì–¼êµ´ ê²€ì¶œì€ ì¼ë°˜ ê°ì²´ë³´ë‹¤ íŒ¨í„´ì´ ë¹„êµì  ëª…í™•í•´, ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ì—ì„œ ì „ì²´ë¥¼ ë¯¸ì„¸ì¡°ì •í•˜ë©´ ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.\n",
        "\n",
        "- í‰ê°€: IoU + AP + mAP@0.5\n",
        "  - íƒì§€ëŠ” \"ë§ì·„ë‹¤/í‹€ë ¸ë‹¤\"ë¥¼ ë‹¨ìˆœ ì •í™•ë„ë¡œ í‘œí˜„í•˜ê¸° ì–´ë µë‹¤. IoUë¡œ â€œê²¹ì¹¨ ì •ë„â€ë¥¼ ê¸°ì¤€ ì‚¼ê³ , ê·¸ ê¸°ì¤€ ìœ„ì—ì„œ AP/mAPë¡œ ì¢…í•© ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.\n",
        "\n",
        "- ë°ì´í„° ë¶ˆê· í˜• ëŒ€ì‘\n",
        "  - WHY: ì‹¤ì œë¡œ cat/dog ê°œìˆ˜ê°€ ì°¨ì´ê°€ ë‚˜ì„œ(ì²´ê°ìƒ 2ë°° ìˆ˜ì¤€), í•™ìŠµì´ í•œ ìª½ í´ë˜ìŠ¤ì— ì¹˜ìš°ì¹  ê°€ëŠ¥ì„±ì´ ìˆë‹¤. ê·¸ë˜ì„œ WeightedRandomSamplerë¡œ ìƒ˜í”Œë§ ë‹¨ê³„ì—ì„œ ê· í˜•ì„ ë³´ì •í•œë‹¤.\n",
        "\n"
      ],
      "metadata": {
        "id": "bsxhd0mcfYQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "â–«ï¸ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ & ì„í¬íŠ¸"
      ],
      "metadata": {
        "id": "Or5GNC4wvcyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torchvision detection: SSD300 êµ¬í˜„ì´ ì•ˆì •ì ìœ¼ë¡œ ì œê³µë¨\n",
        "# transforms.v2 + tv_tensors: bbox(ë°•ìŠ¤)ê¹Œì§€ ê°™ì´ ì•ˆì „í•˜ê²Œ ì¦ê°•í•˜ë ¤ê³  ì‚¬ìš©\n",
        "\n",
        "!pip -q install kagglehub\n",
        "import kagglehub\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms.v2 as v2\n",
        "import torchvision.tv_tensors as tv_tensors\n",
        "\n",
        "from torchvision.models.detection import ssd300_vgg16\n",
        "from torchvision.models.detection.ssd import SSDClassificationHead\n",
        "from torchvision.models.detection.ssd import SSD300_VGG16_Weights\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n"
      ],
      "metadata": {
        "id": "oyYDtRrsvjI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â–«ï¸ë°ì´í„° ë‹¤ìš´ë¡œë“œ & ê²½ë¡œ ì¤€ë¹„"
      ],
      "metadata": {
        "id": "ujh4uhEvvtNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"zippyz/cats-and-dogs-breeds-classification-oxford-dataset\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "image_dir = os.path.join(path, \"images\", \"images\")\n",
        "xml_dir   = os.path.join(path, \"annotations\", \"annotations\", \"xmls\")\n",
        "\n",
        "trainval_txt = os.path.join(path, \"annotations\", \"annotations\", \"trainval.txt\")\n",
        "test_txt     = os.path.join(path, \"annotations\", \"annotations\", \"test.txt\")\n"
      ],
      "metadata": {
        "id": "RblJUXasvzOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â–«ï¸ë°ì´í„° ë¶„í•  íŒŒì¼(trainval/test) ì½ê³  ì •í•©ì„± ì²´í¬"
      ],
      "metadata": {
        "id": "2t4JTQoGv8ON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# íƒì§€ì—ì„œ í”í•œ ì—ëŸ¬: \"ì´ë¯¸ì§€ëŠ” ìˆëŠ”ë° xmlì´ ì—†ê±°ë‚˜\", \"xmlì€ ìˆëŠ”ë° ì´ë¯¸ì§€ê°€ ì—†ëŠ”\" ì¼€ì´ìŠ¤\n",
        "# í•™ìŠµ ì¤‘ê°„ì— í„°ì§€ê¸° ì „ì—, ì´ˆë°˜ì— ë°ì´í„° ì •í•©ì„±ì„ ë¨¼ì € ê²€ì‚¬í•˜ëŠ” ê²Œ ì•ˆì „í•˜ë‹¤.\n",
        "\n",
        "df_trainval = pd.read_csv(trainval_txt, sep=r\"\\s+\", header=None)\n",
        "df_trainval.columns = [\"Image\", \"ClassID\", \"Species\", \"BreedID\"]\n",
        "\n",
        "df_test = pd.read_csv(test_txt, sep=r\"\\s+\", header=None)\n",
        "df_test.columns = [\"Image\", \"ClassID\", \"Species\", \"BreedID\"]\n",
        "\n",
        "print(\"Train/Validation ë°ì´í„° ìˆ˜:\", len(df_trainval))\n",
        "print(\"Test ë°ì´í„° ìˆ˜:\", len(df_test))\n",
        "\n",
        "xml_files = [f for f in os.listdir(xml_dir) if f.endswith(\".xml\")]\n",
        "print(\"XML íŒŒì¼ ê°œìˆ˜:\", len(xml_files))\n",
        "\n",
        "\n",
        "trainval_list = df_trainval[\"Image\"].tolist()\n",
        "test_list     = df_test[\"Image\"].tolist()\n",
        "\n",
        "# ì •í•©ì„± ì²´í¬: ì´ë¯¸ì§€/ì–´ë…¸í…Œì´ì…˜ ì¡´ì¬ ì—¬ë¶€\n",
        "def exists_pair(stem):\n",
        "    img_ok = os.path.exists(os.path.join(image_dir, stem + \".jpg\"))\n",
        "    xml_ok = os.path.exists(os.path.join(xml_dir, stem + \".xml\"))\n",
        "    return img_ok, xml_ok\n",
        "\n",
        "# ìƒ˜í”Œ ëª‡ ê°œë§Œ ë¹ ë¥´ê²Œ í™•ì¸\n",
        "for stem in trainval_list[:5]:\n",
        "    print(stem, exists_pair(stem))\n"
      ],
      "metadata": {
        "id": "9QdL2-fVwBL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… XML íŒŒì‹±ì„ ë¨¼ì € í™•ì¸í•˜ëŠ” ì´ìœ \n",
        "\n",
        "- Object Detectionì€ ì •ë‹µì´ \"ë°•ìŠ¤ ì¢Œí‘œ\"ì´ë‹¤.\n",
        "- ì¦‰, XML íŒŒì„œê°€ í‹€ë¦¬ë©´ ëª¨ë¸ì´ ì•„ë¬´ë¦¬ í•™ìŠµë¼ë„ ê²°ê³¼ê°€ ì „ë¶€ ë§ê°€ì§„ë‹¤.\n",
        "\n",
        "ê·¸ë˜ì„œ ì•„ë˜ ê³¼ì •ì€ ë‹¨ìˆœí•œ ì ˆì°¨ê°€ ì•„ë‹ˆë¼,\n",
        "\"í•™ìŠµ í’ˆì§ˆì„ ê²°ì •í•˜ëŠ” í•µì‹¬ ê¸°ë°˜\"ì´ë¼ê³  ë³´ëŠ” ê²Œ ë§ë‹¤.\n",
        "\n",
        "1) XMLì—ì„œ (xmin, ymin, xmax, ymax) ì¢Œí‘œê°€ ì œëŒ€ë¡œ ì½íˆëŠ”ì§€  \n",
        "2) ì´ë¯¸ì§€ ìœ„ì— ë°•ìŠ¤ë¥¼ ì§ì ‘ ê·¸ë ¤ì„œ ëˆˆìœ¼ë¡œ ë§ëŠ”ì§€  \n",
        "3) ë¼ë²¨(cat/dog)ì´ ê¸°ëŒ€ëŒ€ë¡œ ë“¤ì–´ì˜¤ëŠ”ì§€\n"
      ],
      "metadata": {
        "id": "6vqBBn1HwL4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "â–«ï¸XML íŒŒì„œ + ë¼ë²¨ ê·œì¹™ ì •ì˜"
      ],
      "metadata": {
        "id": "X9bt4QowwVmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SSDëŠ” backgroundë¥¼ 0ìœ¼ë¡œ ë‘ëŠ” ê´€ë¡€ê°€ ìˆë‹¤. ì‹¤ì œ ê°ì²´ í´ë˜ìŠ¤ëŠ” 1ë¶€í„° ì‹œì‘í•˜ëŠ”ê²Œ ì•ˆì „í•˜ë‹¤.\n",
        "# torchvision detection ê³„ì—´ ëŒ€ë¶€ë¶„ì´ ì´ ë°©ì‹ì„ ì „ì œë¡œ í•œë‹¤.\n",
        "\n",
        "classes = [\"background\", \"cat\", \"dog\"]  # label id: 0,1,2\n",
        "NUM_CLASSES = len(classes)\n",
        "\n",
        "def parse_xml(xml_path):\n",
        "    \"\"\"\n",
        "    XML íŒŒì¼ì—ì„œ ë°”ìš´ë”©ë°•ìŠ¤ì™€ ë¼ë²¨ì„ ì¶”ì¶œí•œë‹¤.\n",
        "\n",
        "    ë°˜í™˜ í˜•íƒœ(ê´€ë¡€):\n",
        "    - boxes: (N,4) í…ì„œ, ì¢Œí‘œëŠ” [xmin, ymin, xmax, ymax] (XYXY)\n",
        "    - labels: (N,) í…ì„œ, label idëŠ” 1(cat) ë˜ëŠ” 2(dog)\n",
        "    \"\"\"\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    boxes = []\n",
        "    labels = []\n",
        "\n",
        "    # Oxford Pet XML êµ¬ì¡°ëŠ” object íƒœê·¸ì— bboxê°€ ë“¤ì–´ìˆë‹¤.\n",
        "    # species ë“± ë©”íƒ€ë¥¼ ì“°ê¸°ë„ í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” \"cat vs dog\"ë§Œ í•„ìš”.\n",
        "    for obj in root.findall(\"object\"):\n",
        "        name = obj.find(\"name\").text.lower()  # ì˜ˆ: \"cat\" ë˜ëŠ” \"dog\" í˜•íƒœë¡œ ë“¤ì–´ì˜¤ëŠ” ê²½ìš°ê°€ ë§ìŒ\n",
        "\n",
        "        bnd = obj.find(\"bndbox\")\n",
        "        xmin = float(bnd.find(\"xmin\").text)\n",
        "        ymin = float(bnd.find(\"ymin\").text)\n",
        "        xmax = float(bnd.find(\"xmax\").text)\n",
        "        ymax = float(bnd.find(\"ymax\").text)\n",
        "\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # ë¼ë²¨ ë§¤í•‘ (ë°ì´í„°ì— ë”°ë¼ name í‘œê¸° í˜•íƒœê°€ ë‹¤ë¥´ë©´ ì—¬ê¸°ë§Œ ìˆ˜ì •)\n",
        "        if \"cat\" in name:\n",
        "            labels.append(1)\n",
        "        else:\n",
        "            labels.append(2)\n",
        "\n",
        "    boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "    labels = torch.tensor(labels, dtype=torch.int64)\n",
        "    return boxes, labels\n"
      ],
      "metadata": {
        "id": "7XVDNUPUwZBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â–«ï¸ìƒ˜í”Œ 1ì¥ì— ëŒ€í•´ â€œì •ë‹µ ë°•ìŠ¤â€ ì‹œê°í™”ë¡œ sanity check"
      ],
      "metadata": {
        "id": "gxi5i-cOwjby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# íƒì§€ëŠ” ìˆ˜ì¹˜ë³´ë‹¤ \"ì²˜ìŒì—”\" ëˆˆìœ¼ë¡œ í™•ì¸í•˜ëŠ” ê²Œ í›¨ì”¬ ë¹ ë¥´ë‹¤.\n",
        "# ë°•ìŠ¤ê°€ ì–´ê¸‹ë‚˜ë©´ í•™ìŠµì„ epochì„ ë§ì´ ëŒë ¤ë„ ì˜ë¯¸ê°€ ì—†ë‹¤.\n",
        "\n",
        "def show_image_with_boxes(stem):\n",
        "    img_path = os.path.join(image_dir, stem + \".jpg\")\n",
        "    xml_path = os.path.join(xml_dir, stem + \".xml\")\n",
        "\n",
        "    img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "    boxes, labels = parse_xml(xml_path)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "    ax.imshow(img)\n",
        "\n",
        "    for box, lb in zip(boxes, labels):\n",
        "        x1, y1, x2, y2 = box.tolist()\n",
        "        w, h = x2 - x1, y2 - y1\n",
        "        rect = patches.Rectangle((x1, y1), w, h, fill=False, linewidth=2)\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x1, y1 - 5, classes[lb.item()], fontsize=10)\n",
        "\n",
        "    ax.set_title(f\"GT boxes check: {stem}\")\n",
        "    ax.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "show_image_with_boxes(trainval_list[0])\n"
      ],
      "metadata": {
        "id": "GPZcF1EMwmxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Datasetì„ ì´ë ‡ê²Œ êµ¬ì„±í•˜ëŠ” ì´ìœ \n",
        "\n",
        "torchvision detection ëª¨ë¸(SSD í¬í•¨)ì€ ì…ë ¥ì„ ë‹¤ìŒ í˜•íƒœë¡œ ë°›ëŠ”ë‹¤.\n",
        "\n",
        "- images: ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ì´ë¯¸ì§€ í…ì„œë“¤ (ê°ê° [C,H,W])\n",
        "- targets: ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ dictë“¤\n",
        "  - boxes: (N,4) XYXY\n",
        "  - labels: (N,)\n",
        "  - ì¶”ê°€ë¡œ image_id: í‰ê°€ ì‹œ ì¶”ì ì„ ìœ„í•´ ë„£ì–´ì£¼ë©´ í¸í•˜ë‹¤\n",
        "\n",
        "ì—¬ê¸°ì„œ ì¤‘ìš”í•œ í¬ì¸íŠ¸ëŠ”,\n",
        "\"ì´ë¯¸ì§€ ì¦ê°•ì„ í•  ë•Œ ë°•ìŠ¤ë„ ê°™ì´ ë³€í™˜ë˜ì–´ì•¼ í•œë‹¤\"ëŠ” ê²ƒ.\n",
        "ê·¸ë˜ì„œ transforms.v2 + tv_tensors.BoundingBoxesë¥¼ ì‚¬ìš©í•œë‹¤.\n"
      ],
      "metadata": {
        "id": "agkJHChiwtX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "â–«ï¸Detection Dataset + collate_fn"
      ],
      "metadata": {
        "id": "N9YC64Dsw2Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PetFaceDataset(Dataset):\n",
        "    def __init__(self, image_list, transforms=None):\n",
        "        self.image_list = image_list\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        stem = self.image_list[idx]\n",
        "        img_path = os.path.join(image_dir, stem + \".jpg\")\n",
        "        xml_path = os.path.join(xml_dir, stem + \".xml\")\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        boxes, labels = parse_xml(xml_path)\n",
        "\n",
        "        # tv_tensorsë¡œ ë°”ê¿”ì•¼ transforms.v2ê°€ bboxê¹Œì§€ ê°™ì´ ë³€í™˜í•´ì¤€ë‹¤.\n",
        "        image = tv_tensors.Image(image)\n",
        "        bbox  = tv_tensors.BoundingBoxes(\n",
        "            boxes,\n",
        "            format=tv_tensors.BoundingBoxFormat.XYXY,\n",
        "            canvas_size=image.shape[-2:]  # (H,W)\n",
        "        )\n",
        "\n",
        "        if self.transforms:\n",
        "            image, bbox = self.transforms(image, bbox)\n",
        "            boxes = bbox.data\n",
        "\n",
        "        target = {\n",
        "            \"image_id\": torch.tensor(idx),  # í‰ê°€ ë•Œ ì–´ë–¤ ì´ë¯¸ì§€ì¸ì§€ ì¶”ì ìš©(í¸ì˜)\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # detectionì€ ì´ë¯¸ì§€ë§ˆë‹¤ ê°ì²´ ìˆ˜ê°€ ë‹¬ë¼ì„œ í…ì„œë¡œ stackì´ ì•ˆ ëœë‹¤. ê·¸ë˜ì„œ \"ë¦¬ìŠ¤íŠ¸\"ë¡œ ë¬¶ì–´ì„œ ë°˜í™˜í•˜ëŠ” collate_fnì´ í•„ìš”í•˜ë‹¤.\n",
        "    images, targets = tuple(zip(*batch))\n",
        "    return list(images), list(targets)\n"
      ],
      "metadata": {
        "id": "WWmtdyxKw44J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â–«ï¸Transform ì •ì˜(Train vs Val/Test)"
      ],
      "metadata": {
        "id": "kpzkSIkVw90k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train: ë‹¤ì–‘í•œ í™˜ê²½(ë°ê¸°, êµ¬ë„)ì„ í‰ë‚´ë‚´ì„œ ì¼ë°˜í™”ë¥¼ ì¡°ê¸ˆì´ë¼ë„ ì˜¬ë¦¬ê¸°\n",
        "# Val/Test: ëœë¤ ì¦ê°• ì—†ì´, \"í‰ê°€ ê°€ëŠ¥í•œ ê³ ì • ì „ì²˜ë¦¬\"ë§Œ ì ìš©\n",
        "\n",
        "train_transform = v2.Compose([\n",
        "    v2.RandomResizedCrop((300, 300), scale=(0.7, 1.0)),\n",
        "    v2.RandomPerspective(distortion_scale=0.3, p=0.5),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(dtype=torch.float32, scale=True),\n",
        "    v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "test_transform = v2.Compose([\n",
        "    v2.Resize((300, 300)),\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(dtype=torch.float32, scale=True),\n",
        "    v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "])\n"
      ],
      "metadata": {
        "id": "LaCwXYguxAix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â–«ï¸Train/Val ë¶„í•  + í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì •(WeightedRandomSampler)"
      ],
      "metadata": {
        "id": "qWdoVWfMxD6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê°„ë‹¨í•˜ê²Œ trainvalì„ train/valë¡œ ë‚˜ëˆ”\n",
        "full_dataset = PetFaceDataset(trainval_list, transforms=train_transform)\n",
        "\n",
        "val_ratio = 0.2\n",
        "val_size = int(len(full_dataset) * val_ratio)\n",
        "train_size = len(full_dataset) - val_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "# Valì€ ì¦ê°•ì„ ë¹¼ê³  ê³ ì • ì „ì²˜ë¦¬ë¡œ ë‹¤ì‹œ ê°ì‹¸ëŠ” ê²Œ ì•ˆì „\n",
        "# valì— ëœë¤ ì¦ê°•ì´ ë“¤ì–´ê°€ë©´ \"ì¸¡ì •ê°’\"ì´ í”ë“¤ë ¤ì„œ ë¹„êµê°€ ì–´ë ¤ì›Œì§„ë‹¤.\n",
        "val_dataset.dataset.transforms = test_transform\n",
        "\n",
        "\n",
        "# í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì •\n",
        "# íŠ¹ì • í´ë˜ìŠ¤ê°€ ë” ë§ìœ¼ë©´, ëª¨ë¸ì´ ê·¸ìª½ë§Œ ë§ì¶”ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì´ ê¸°ìš¸ ìˆ˜ ìˆë‹¤.\n",
        "def get_sample_weights(dataset_subset):\n",
        "    class_counts = {1: 0, 2: 0}  # cat, dog\n",
        "\n",
        "    # subsetì€ random_splitì˜ Subset ê°ì²´ë¼ì„œ dataset_subset[i]ë¡œ ì ‘ê·¼í•˜ë©´ (image, target)ì´ ë‚˜ì˜¨ë‹¤.\n",
        "    for i in range(len(dataset_subset)):\n",
        "        _, target = dataset_subset[i]\n",
        "        for lb in target[\"labels\"]:\n",
        "            if lb.item() in class_counts:\n",
        "                class_counts[lb.item()] += 1\n",
        "\n",
        "    # ì—­ë¹ˆë„ ê¸°ë°˜ weight\n",
        "    total = sum(class_counts.values())\n",
        "    class_weights = {k: (total / v) for k, v in class_counts.items() if v > 0}\n",
        "\n",
        "    sample_weights = []\n",
        "    for i in range(len(dataset_subset)):\n",
        "        _, target = dataset_subset[i]\n",
        "        main_class = target[\"labels\"][0].item()  # ì´ ë°ì´í„°ëŠ” ë³´í†µ 1ê°œ ê°ì²´(ì–¼êµ´)ë¼ 0ë²ˆì§¸ë¡œ ì¶©ë¶„\n",
        "        sample_weights.append(class_weights.get(main_class, 1.0))\n",
        "\n",
        "    return torch.DoubleTensor(sample_weights), class_counts\n",
        "\n",
        "\n",
        "sample_weights, class_counts = get_sample_weights(train_dataset)\n",
        "print(\"class_counts (train):\", class_counts)\n",
        "\n",
        "\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n"
      ],
      "metadata": {
        "id": "PBRSmx89xHbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â–«ï¸DataLoader ìƒì„±"
      ],
      "metadata": {
        "id": "zZa0veeQxbbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# test datasetì€ test_list ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„±\n",
        "test_dataset = PetFaceDataset(test_list, transforms=test_transform)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(\"loaders ready\")\n"
      ],
      "metadata": {
        "id": "n5OYzTsTxbvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… SSD ëª¨ë¸ì—ì„œ â€œë¶„ë¥˜ í—¤ë“œ êµì²´â€ê°€ ì™œ í•„ìš”í•œê°€?\n",
        "\n",
        "ì‚¬ì „í•™ìŠµ SSD300(VGG16)ì€ COCO ê°™ì€ ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ ìˆ˜ì— ë§ì¶°ì ¸ ìˆë‹¤.\n",
        "í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” \"background / cat / dog\" 3í´ë˜ìŠ¤ë§Œ í•„ìš”í•˜ë‹¤.\n",
        "\n",
        "ë”°ë¼ì„œ í•´ì•¼ í•  í•µì‹¬ ì‘ì—…ì€ ë”± í•˜ë‚˜ë‹¤.\n",
        "- ë¶„ë¥˜ í—¤ë“œ(classification head)ë§Œ 3í´ë˜ìŠ¤ ì¶œë ¥ìœ¼ë¡œ êµì²´í•œë‹¤.\n",
        "\n",
        "ì´ê±¸ ì•ˆ í•˜ë©´,\n",
        "ëª¨ë¸ì€ ì—‰ëš±í•œ í´ë˜ìŠ¤ ê³µê°„ì—ì„œ í•™ìŠµí•˜ê²Œ ë˜ì–´ lossê°€ ì˜ë¯¸ê°€ ì—†ì–´ì§€ê±°ë‚˜ í•™ìŠµì´ ì˜ ì•ˆ ëœë‹¤.\n"
      ],
      "metadata": {
        "id": "76ng_0D0xgZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "â–«ï¸SSD300(VGG16) ë¡œë“œ + classification head êµì²´"
      ],
      "metadata": {
        "id": "5b9mjoUrxpWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 3  # background, cat, dog\n",
        "\n",
        "model = ssd300_vgg16(weights=SSD300_VGG16_Weights.DEFAULT)\n",
        "\n",
        "# SSD300ì˜ feature mapë³„ ì±„ë„ ìˆ˜ëŠ” ê´€ë¡€ì ìœ¼ë¡œ ì•„ë˜ì²˜ëŸ¼ ì‚¬ìš©\n",
        "in_channels = [512, 1024, 512, 256, 256, 256]\n",
        "num_anchors = model.anchor_generator.num_anchors_per_location()\n",
        "\n",
        "# ë¶„ë¥˜ í—¤ë“œ êµì²´\n",
        "model.head.classification_head = SSDClassificationHead(in_channels, num_anchors, num_classes)\n",
        "\n",
        "model.to(device)\n",
        "print(\"SSD300_VGG16 ready for fine-tuning\")\n"
      ],
      "metadata": {
        "id": "2blXOumzxq6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… mAPë¥¼ ì™œ ì§ì ‘ êµ¬í˜„í–ˆë‚˜?\n",
        "- baselineì—ì„œ APë¥¼ scikit-learn average_precision_scoreë¡œ ë‹¨ìˆœíˆ ê³„ì‚°í•˜ë©´\n",
        "\"íƒì§€ ë¬¸ì œì˜ ì •ì„ í‰ê°€\"ì™€ ë‹¤ë¥´ê²Œ ë™ì‘í•  ìˆ˜ ìˆì–´ ì°œì°œí•œ ë¶€ë¶„ì´ ìƒê¸´ë‹¤.\n",
        "\n",
        "íƒì§€ì˜ AP/mAPëŠ” ë³´í†µ ë‹¤ìŒ ì ˆì°¨ë¥¼ ë”°ë¥¸ë‹¤.\n",
        "\n",
        "1) ì˜ˆì¸¡ ë°•ìŠ¤ë“¤ì„ score ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬  \n",
        "2) ê° ì˜ˆì¸¡ì´ GT ë°•ìŠ¤ì™€ IoU ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­ë˜ëŠ”ì§€ íŒë‹¨  \n",
        "3) TP/FP ëˆ„ì  â†’ Precision/Recall ì»¤ë¸Œ ìƒì„±  \n",
        "4) ë©´ì (AP) ê³„ì‚° â†’ í´ë˜ìŠ¤ë³„ AP í‰ê· ì´ mAP\n",
        "\n",
        "- ì¦‰, íƒì§€ í‰ê°€ëŠ” \"ë°•ìŠ¤ ë§¤ì¹­ ë¡œì§\"ì´ í•µì‹¬ì´ë¼, ì´ë¥¼ ì§ì ‘ êµ¬í˜„í•´ë‘ë©´ ê²°ê³¼ë¥¼ í›¨ì”¬ ë” ë‚©ë“í•˜ê¸° ì‰½ë‹¤.\n"
      ],
      "metadata": {
        "id": "X593qniWxx6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "â–«ï¸IoU + AP + mAP@0.5 í‰ê°€ í•¨ìˆ˜"
      ],
      "metadata": {
        "id": "UlPsOm-ux8Ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou_numpy(box1, box2):\n",
        "    \"\"\"\n",
        "    IoU(Intersection over Union) ì˜ˆì‹œë¡œ ì´í•´í•˜ê¸°:\n",
        "    - box1ê³¼ box2ê°€ ì–¼ë§ˆë‚˜ 'ê²¹ì¹˜ëŠ”ì§€'ë¥¼ 0~1ë¡œ ìˆ˜ì¹˜í™”í•œ ê°’\n",
        "    - ì™„ì „ ê²¹ì¹˜ë©´ 1, ì „í˜€ ì•ˆ ê²¹ì¹˜ë©´ 0\n",
        "\n",
        "    box: [x1,y1,x2,y2] (XYXY)\n",
        "    \"\"\"\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    inter_w = max(0.0, x2 - x1)\n",
        "    inter_h = max(0.0, y2 - y1)\n",
        "    inter_area = inter_w * inter_h\n",
        "\n",
        "    area1 = max(0.0, (box1[2]-box1[0])) * max(0.0, (box1[3]-box1[1]))\n",
        "    area2 = max(0.0, (box2[2]-box2[0])) * max(0.0, (box2[3]-box2[1]))\n",
        "\n",
        "    union = area1 + area2 - inter_area + 1e-9\n",
        "    return inter_area / union\n",
        "\n",
        "def voc_ap(rec, prec):\n",
        "    \"\"\"\n",
        "    VOC ë°©ì‹ AP ê³„ì‚°:\n",
        "    - Precision-Recall ê³¡ì„  ì•„ë˜ ë©´ì ì„ ê³„ì‚°\n",
        "    - êµ¬í˜„ í¬ì¸íŠ¸: precisionì„ ì˜¤ë¥¸ìª½ì—ì„œ ì™¼ìª½ìœ¼ë¡œ monotonicí•˜ê²Œ ë³´ì •\n",
        "    \"\"\"\n",
        "    mrec  = np.concatenate(([0.0], rec,  [1.0]))\n",
        "    mpre  = np.concatenate(([0.0], prec, [0.0]))\n",
        "\n",
        "    for i in range(len(mpre)-1, 0, -1):\n",
        "        mpre[i-1] = max(mpre[i-1], mpre[i])\n",
        "\n",
        "    idx = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "    ap = np.sum((mrec[idx+1] - mrec[idx]) * mpre[idx+1])\n",
        "    return ap\n",
        "\n",
        "def evaluate_map(preds, gts, iou_threshold=0.5, num_classes=3):\n",
        "    \"\"\"\n",
        "    preds: list of dict\n",
        "      - image_id, bbox(XYXY), score, label(1 or 2)\n",
        "    gts: list of dict\n",
        "      - image_id, boxes(N,4), labels(N,)\n",
        "    \"\"\"\n",
        "    # GTë¥¼ image_id ê¸°ì¤€ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì°¾ê¸° ìœ„í•œ dict\n",
        "    gt_by_image = {}\n",
        "    for gt in gts:\n",
        "        gt_by_image[gt[\"image_id\"]] = {\n",
        "            \"boxes\": gt[\"boxes\"],\n",
        "            \"labels\": gt[\"labels\"],\n",
        "            \"detected\": {1: set(), 2: set()}  # í´ë˜ìŠ¤ë³„ë¡œ ì–´ë–¤ GTê°€ ë§¤ì¹­ëëŠ”ì§€ ê¸°ë¡\n",
        "        }\n",
        "\n",
        "    aps = []\n",
        "    for cls in range(1, num_classes):  # 1~(num_classes-1): cat,dog\n",
        "        cls_preds = [p for p in preds if p[\"label\"] == cls]\n",
        "        cls_preds = sorted(cls_preds, key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "        tp = np.zeros(len(cls_preds))\n",
        "        fp = np.zeros(len(cls_preds))\n",
        "\n",
        "        # ì´ í´ë˜ìŠ¤ì˜ GT ê°œìˆ˜\n",
        "        n_gt = 0\n",
        "        for gt in gts:\n",
        "            n_gt += np.sum(gt[\"labels\"] == cls)\n",
        "\n",
        "        for i, p in enumerate(cls_preds):\n",
        "            image_id = p[\"image_id\"]\n",
        "            pred_box = p[\"bbox\"]\n",
        "\n",
        "            if image_id not in gt_by_image:\n",
        "                fp[i] = 1\n",
        "                continue\n",
        "\n",
        "            gt_info = gt_by_image[image_id]\n",
        "            gt_boxes = gt_info[\"boxes\"]\n",
        "            gt_labels = gt_info[\"labels\"]\n",
        "\n",
        "            # ì´ ì´ë¯¸ì§€ì—ì„œ ì´ í´ë˜ìŠ¤ GTë§Œ ë½‘ê¸°\n",
        "            cls_gt_idx = np.where(gt_labels == cls)[0]\n",
        "            if len(cls_gt_idx) == 0:\n",
        "                fp[i] = 1\n",
        "                continue\n",
        "\n",
        "            # IoU ìµœëŒ€ ë§¤ì¹­ ì°¾ê¸°\n",
        "            best_iou = 0.0\n",
        "            best_j = -1\n",
        "            for j in cls_gt_idx:\n",
        "                iou = compute_iou_numpy(pred_box, gt_boxes[j])\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_j = int(j)\n",
        "\n",
        "            # IoU ê¸°ì¤€ ì¶©ì¡± + ì•„ì§ ë§¤ì¹­ ì•ˆ ëœ GTë©´ TP\n",
        "            if best_iou >= iou_threshold and best_j not in gt_info[\"detected\"][cls]:\n",
        "                tp[i] = 1\n",
        "                gt_info[\"detected\"][cls].add(best_j)\n",
        "            else:\n",
        "                fp[i] = 1\n",
        "\n",
        "        # ëˆ„ì í•©ìœ¼ë¡œ PR ê³„ì‚°\n",
        "        tp_cum = np.cumsum(tp)\n",
        "        fp_cum = np.cumsum(fp)\n",
        "\n",
        "        rec = tp_cum / (n_gt + 1e-9)\n",
        "        prec = tp_cum / (tp_cum + fp_cum + 1e-9)\n",
        "\n",
        "        ap = voc_ap(rec, prec)\n",
        "        aps.append(ap)\n",
        "\n",
        "    mAP = float(np.mean(aps)) if len(aps) > 0 else 0.0\n",
        "    return mAP, aps\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sqLLh7TMx9ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸŸ© í•™ìŠµ/ê²€ì¦ ë£¨í”„"
      ],
      "metadata": {
        "id": "ENOZz0VLyD6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Fine-tuning: backbone í¬í•¨ ì „ì²´ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸\n",
        "# ê·¸ë˜ì„œ lrì€ ë„ˆë¬´ í¬ê²Œ ì¡ìœ¼ë©´ ì‰½ê²Œ ë°œì‚°/ë¶ˆì•ˆì •í•´ì§ˆ ìˆ˜ ìˆë‹¤.\n",
        "# AdamW: weight decayë¡œ ê³¼ì í•© ì™„í™”ì— ë„ì›€ì„ ê¸°ëŒ€\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "num_epochs = 50\n",
        "best_map = 0.0\n",
        "\n",
        "train_losses, val_losses, val_maps = [], [], []\n",
        "\n",
        "# val GTë¥¼ ë¯¸ë¦¬ ë½‘ì•„ë‘ë©´, ë§¤ epoch í‰ê°€ê°€ ë‹¨ìˆœí•´ì§\n",
        "ground_truths_val = []\n",
        "for _, target in val_dataset:\n",
        "    ground_truths_val.append({\n",
        "        \"image_id\": int(target[\"image_id\"].item()),\n",
        "        \"boxes\": target[\"boxes\"].numpy(),\n",
        "        \"labels\": target[\"labels\"].numpy()\n",
        "    })\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Train\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, targets in train_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) if torch.is_tensor(v) else v for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        # SSDëŠ” ì—¬ëŸ¬ loss(ë¶„ë¥˜ + ìœ„ì¹˜)ë¥¼ dictë¡œ ì¤€ë‹¤ â†’ í•©ì³ì„œ ìµœì¢… lossë¡œ ì‚¬ìš©\n",
        "        loss = sum(loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / max(1, len(train_loader))\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "\n",
        "    # Val (loss + mAP)\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    predictions_val = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in val_loader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets_gpu = [{k: v.to(device) if torch.is_tensor(v) else v for k, v in t.items()} for t in targets]\n",
        "\n",
        "            # val lossë„ ë³´ê³  ì‹¶ìœ¼ë©´(ë„¤ ë¶„ì„ì²˜ëŸ¼) trainê³¼ ë™ì¼í•˜ê²Œ loss ê³„ì‚°\n",
        "            loss_dict = model(images, targets_gpu)\n",
        "            loss = sum(loss_dict.values())\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # ì˜ˆì¸¡ì€ target ì—†ì´ forward\n",
        "            outputs = model(images)\n",
        "\n",
        "            # outputs: ê° ì´ë¯¸ì§€ë§ˆë‹¤ dict(boxes, labels, scores)\n",
        "            for i, output in enumerate(outputs):\n",
        "                img_id = int(targets[i][\"image_id\"].item())\n",
        "                for j in range(len(output[\"boxes\"])):\n",
        "                    predictions_val.append({\n",
        "                        \"image_id\": img_id,\n",
        "                        \"bbox\": output[\"boxes\"][j].cpu().numpy(),\n",
        "                        \"score\": float(output[\"scores\"][j].cpu().numpy()),\n",
        "                        \"label\": int(output[\"labels\"][j].cpu().numpy()),\n",
        "                    })\n",
        "\n",
        "    avg_val_loss = val_loss / max(1, len(val_loader))\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    val_map, aps = evaluate_map(predictions_val, ground_truths_val, iou_threshold=0.5, num_classes=NUM_CLASSES)\n",
        "    val_maps.append(val_map)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val mAP: {val_map:.4f}\")\n",
        "\n",
        "    # Best ëª¨ë¸ ì €ì¥\n",
        "    if val_map > best_map:\n",
        "        best_map = val_map\n",
        "        torch.save(model.state_dict(), \"best_ssd_model.pth\")\n",
        "        print(f\"Best mAP Updated: {best_map:.4f}. Model Saved\")\n",
        "\n",
        "print(\"í•™ìŠµ ì™„ë£Œ\")\n",
        "print(f\"ìµœì¢… Best mAP: {best_map:.4f}\")\n",
        "\n",
        "\n",
        "'''\n",
        "Epoch 1: Val mAP 0.1919\n",
        "\n",
        "Epoch 5: Val mAP 0.9488\n",
        "\n",
        "Epoch 10: Val mAP 0.9784\n",
        "\n",
        "Epoch 34: Best mAP 0.9939\n",
        "\n",
        "Epoch 50: Val mAP 0.9930\n",
        "\n",
        "ìµœì¢… ì¶œë ¥: ìµœì¢… Best mAP: 0.9939\n",
        "'''"
      ],
      "metadata": {
        "id": "8Hj5iU5KyLqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â–«ï¸í•™ìŠµ ê³¡ì„  ì‹œê°í™” (loss / mAP)"
      ],
      "metadata": {
        "id": "b_K3r5SVyaFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"mAPëŠ” ì¢‹ì€ë° lossê°€ ìƒê°ë³´ë‹¤ ì•ˆ ë‚´ë ¤ê°„ë‹¤\" ê°™ì€ ìƒí™©ì´ ìˆë‹¤.\n",
        "# íƒì§€ëŠ” lossê°€ ì—¬ëŸ¬ ê°œ(ë¶„ë¥˜+ìœ„ì¹˜)ë¼ì„œ, loss ìì²´ë³´ë‹¤ mAPê°€ ë” ëª©ì ì— ê°€ê¹ë‹¤.\n",
        "# ê·¸ë˜ë„ ê³¡ì„ ì„ ë³´ë©´ í•™ìŠµì´ ì •ìƒì ìœ¼ë¡œ ì§„í–‰ë˜ëŠ”ì§€ ê°ì„ ì¡ì„ ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_losses, label=\"train loss\")\n",
        "plt.plot(val_losses, label=\"val loss\")\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(val_maps, label=\"val mAP@0.5\")\n",
        "plt.title(\"mAP\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"best mAP:\", best_map)\n"
      ],
      "metadata": {
        "id": "lFgkDVG5ydDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â–«ï¸í…ŒìŠ¤íŠ¸ì…‹ ì˜ˆì¸¡ ì‹œê°í™”(ì •ì„± í‰ê°€)"
      ],
      "metadata": {
        "id": "1v6w_HL4yhdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ\n",
        "model.load_state_dict(torch.load(\"best_ssd_model.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "def denormalize(img_tensor):\n",
        "    # v2.Normalizeë¥¼ ë˜ëŒë ¤ì„œ ì‚¬ëŒì´ ë³´ê¸° ì¢‹ê²Œ ë§Œë“œëŠ” í•¨ìˆ˜\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406], device=img_tensor.device).view(3,1,1)\n",
        "    std  = torch.tensor([0.229, 0.224, 0.225], device=img_tensor.device).view(3,1,1)\n",
        "    x = img_tensor * std + mean\n",
        "    x = torch.clamp(x, 0, 1)\n",
        "    return x\n",
        "\n",
        "@torch.no_grad()\n",
        "def show_test_predictions(n_images=8, score_thr=0.5):\n",
        "    shown = 0\n",
        "    for images, targets in test_loader:\n",
        "        images_gpu = [img.to(device) for img in images]\n",
        "        outputs = model(images_gpu)\n",
        "\n",
        "        for img_t, out in zip(images, outputs):\n",
        "            img_np = denormalize(img_t.to(device)).permute(1,2,0).cpu().numpy()\n",
        "\n",
        "            fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
        "            ax.imshow(img_np)\n",
        "            ax.axis(\"off\")\n",
        "\n",
        "            for box, score, label in zip(out[\"boxes\"], out[\"scores\"], out[\"labels\"]):\n",
        "                if float(score) < score_thr:\n",
        "                    continue\n",
        "                x1,y1,x2,y2 = box.cpu().numpy()\n",
        "                rect = patches.Rectangle((x1,y1), x2-x1, y2-y1, fill=False, linewidth=2)\n",
        "                ax.add_patch(rect)\n",
        "                ax.text(x1, y1-5, f\"{classes[int(label)]} {float(score):.2f}\", fontsize=10)\n",
        "\n",
        "            plt.show()\n",
        "            shown += 1\n",
        "            if shown >= n_images:\n",
        "                return\n",
        "\n",
        "show_test_predictions(n_images=8, score_thr=0.5)\n"
      ],
      "metadata": {
        "id": "4NLluN9LylMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸŸ¨ ìµœì¢… ê²°ë¡  ë° ë¶„ì„\n",
        "\n",
        "## 1. ì‹¤í—˜ ìš”ì•½\n",
        "ë³¸ ì‹¤í—˜ì€ Oxford-IIIT Pet ë°ì´í„°ì˜ XML ë°”ìš´ë”©ë°•ìŠ¤ë¥¼ ì´ìš©í•´,\n",
        "SSD300(VGG16) ê¸°ë°˜ìœ¼ë¡œ ê°œ/ê³ ì–‘ì´ ì–¼êµ´ ì˜ì—­ì„ ê²€ì¶œí•˜ëŠ” íƒì§€ ëª¨ë¸ì„ êµ¬ì¶•í–ˆë‹¤.\n",
        "\n",
        "íŒŒì´í”„ë¼ì¸ì€ ë‹¤ìŒ ìˆœì„œë¡œ êµ¬ì„±í–ˆë‹¤.\n",
        "- ë°ì´í„° ì •í•©ì„± í™•ì¸(ì´ë¯¸ì§€-XML ë§¤ì¹­)\n",
        "- XML íŒŒì‹± ê²€ì¦(ì‹œê°í™” sanity check)\n",
        "- detection dataset/dataloader êµ¬ì„±(ë°•ìŠ¤ ë³€í™˜ í¬í•¨)\n",
        "- í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì •(WeightedRandomSampler)\n",
        "- SSD300 ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ í›„ ë¶„ë¥˜ í—¤ë“œ êµì²´(3í´ë˜ìŠ¤)\n",
        "- Full Fine-tuning í•™ìŠµ + mAP@0.5 í‰ê°€ + í…ŒìŠ¤íŠ¸ ì •ì„± í™•ì¸\n",
        "\n",
        "---\n",
        "\n",
        "## 2. ì •ëŸ‰ ê²°ê³¼\n",
        "í•™ìŠµì´ ì§„í–‰ë˜ë©´ì„œ mAPê°€ ë¹ ë¥´ê²Œ ìƒìŠ¹í–ˆê³ , ì´í›„ 0.99 ìˆ˜ì¤€ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ìœ ì§€ë˜ëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤.\n",
        "\n",
        "- Epoch 1: Val mAP 0.1919\n",
        "- Epoch 5: Val mAP 0.9488\n",
        "- Epoch 10: Val mAP 0.9784\n",
        "- Epoch 34: Best Val mAP 0.9939\n",
        "- Epoch 50: Val mAP 0.9930\n",
        "- ìµœì¢… Best mAP: 0.9939\n",
        "\n",
        "ë˜í•œ train lossëŠ” ëŒ€ëµ 6.93 â†’ 1.34 ìˆ˜ì¤€ìœ¼ë¡œ ê°ì†Œí–ˆê³ ,\n",
        "val lossëŠ” 1.92ëŒ€(ìµœì €)ê¹Œì§€ ë‚´ë ¤ê°€ë©° ì „ì²´ì ìœ¼ë¡œ í•™ìŠµì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜ë ´í–ˆë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. ê´€ì°° ë° í•´ì„\n",
        "1) mAPëŠ” ë§¤ìš° ë†’ê²Œ ë‚˜ì™”ì§€ë§Œ, lossì™€ ì™„ì „íˆ ê°™ì€ ë°©í–¥ìœ¼ë¡œ ì›€ì§ì´ì§€ëŠ” ì•Šì•˜ë‹¤  \n",
        "- íƒì§€ ëª¨ë¸ì˜ lossëŠ” (ë¶„ë¥˜ ì†ì‹¤ + ìœ„ì¹˜ ì†ì‹¤) ë“± ì—¬ëŸ¬ ìš”ì†Œê°€ í•©ì³ì§„ ê°’ì´ë¼\n",
        "  â€œlossê°€ ì¡°ê¸ˆ ë†’ì€ë°ë„ mAPê°€ ì¢‹ì•„ì§€ëŠ”â€ ìƒí™©ì´ ì¶©ë¶„íˆ ë°œìƒí•  ìˆ˜ ìˆë‹¤.\n",
        "- ë”°ë¼ì„œ ì´ ë¬¸ì œì˜ ëª©ì (ì–¼êµ´ ê²€ì¶œ ì„±ëŠ¥) ê´€ì ì—ì„œëŠ” mAPë¥¼ 1ìˆœìœ„ë¡œ ë‘ëŠ” íŒë‹¨ì´ íƒ€ë‹¹í•˜ë‹¤.\n",
        "\n",
        "2) ë°ì´í„° ì •í•©ì„± ê²€ì‚¬ì™€ ì´ˆê¸° ì‹œê°í™”ê°€ ì‹¤í—˜ í’ˆì§ˆì— ê²°ì •ì ì´ì—ˆë‹¤  \n",
        "- ì´ë¯¸ì§€-XML ë¶ˆì¼ì¹˜ê°€ ì„ì´ë©´ í•™ìŠµ ìì²´ê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,\n",
        "  ì´ˆë°˜ì— ì •í•©ì„± ê²€ì‚¬ë¥¼ ìˆ˜í–‰í•œ ì ‘ê·¼ì€ ì‹¤ì „ì—ì„œë„ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤.\n",
        "\n",
        "3) í´ë˜ìŠ¤ ë¶ˆê· í˜•ì€ ìƒ˜í”Œë§ ë‹¨ê³„ì—ì„œ ë¨¼ì € ì™„í™”í•˜ëŠ” í¸ì´ ì•ˆì „í–ˆë‹¤  \n",
        "- ë°ì´í„°ê°€ í•œ í´ë˜ìŠ¤ì— ì¹˜ìš°ì¹˜ë©´ ëª¨ë¸ì´ ì‰¬ìš´ ë°©í–¥ìœ¼ë¡œ í•™ìŠµì´ ê¸°ìš¸ ìˆ˜ ìˆë‹¤.\n",
        "- WeightedRandomSamplerëŠ” ëª¨ë¸ êµ¬ì¡°ë¥¼ ë°”ê¾¸ì§€ ì•Šê³ ë„ í•™ìŠµ ë¶„í¬ë¥¼ ë³´ì •í•  ìˆ˜ ìˆì–´ ì ìš© ê°€ì¹˜ê°€ ìˆë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. í•œê³„ ë° ê°œì„  ë°©í–¥\n",
        "- mAPê°€ ë†’ì€ ì´ìœ ê°€ â€œë¬¸ì œ ë‚œì´ë„(ì–¼êµ´ì´ ë¹„êµì  í¬ê³  ëª…í™•)â€ì˜ ì˜í–¥ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,\n",
        "  ë” ì–´ë ¤ìš´ ì¡°ê±´(ì‘ì€ ì–¼êµ´, ê°€ë¦¼, ë‹¤ì–‘í•œ ì¡°ëª…)ì—ì„œì˜ ì¼ë°˜í™” í…ŒìŠ¤íŠ¸ê°€ ì¶”ê°€ë¡œ í•„ìš”í•˜ë‹¤.\n",
        "- í–¥í›„ ê°œì„  ë°©í–¥\n",
        "  - score threshold ì¡°ì • + NMS ì„¤ì • ì ê²€ìœ¼ë¡œ FPë¥¼ ë” ì¤„ì´ëŠ” ë°©í–¥\n",
        "  - ë” ê°•í•œ ì¦ê°•(ColorJitter, Blur ë“±)ì˜ ì˜í–¥ ë¹„êµ\n",
        "  - IoU threshold(0.5 â†’ 0.75 ë“±)ì—ì„œ ì„±ëŠ¥ì´ ìœ ì§€ë˜ëŠ”ì§€ ì¶”ê°€ ì ê²€\n",
        "\n",
        "---\n",
        "\n",
        "## 5. ê²°ë¡ \n",
        "SSD300 ê¸°ë°˜ ê°œ/ê³ ì–‘ì´ ì–¼êµ´ íƒì§€ íŒŒì´í”„ë¼ì¸ì„ end-to-endë¡œ êµ¬ì¶•í–ˆìœ¼ë©°,\n",
        "mAP@0.5 ê¸°ì¤€ ìµœê³  0.9939ì˜ ì„±ëŠ¥ì„ í™•ì¸í–ˆë‹¤.\n",
        "ë˜í•œ í‰ê°€ ë¡œì§(AP/mAP)ì„ ì§ì ‘ êµ¬ì„±í•˜ì—¬ ê²°ê³¼ì— ëŒ€í•œ ë‚©ë“ ê°€ëŠ¥ì„±ê³¼ ì¬í˜„ì„±ì„ í™•ë³´í–ˆë‹¤.\n"
      ],
      "metadata": {
        "id": "ABsIWluCyqOM"
      }
    }
  ]
}